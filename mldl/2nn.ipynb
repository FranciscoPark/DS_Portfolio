{"cells":[{"cell_type":"markdown","metadata":{"id":"sPEoabX-hGCh"},"source":["# Import Modules"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"CfcNKryaj6VJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/PATH/TO/MNIST_DATA_FILE'"],"metadata":{"id":"n6CbjnM5kbBO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OyammZP8hI7P"},"outputs":[],"source":["import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score\n","from mnist.data_utils import load_data"]},{"cell_type":"markdown","metadata":{"id":"iLxTNOvI5NHD"},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xuQB6W2U5ZE2"},"outputs":[],"source":["def elu(z, alpha=1.0):\n","    \"\"\"\n","    Implement the elu activation function.\n","    The method takes the input z and returns the output of the function.\n","    Please DO NOT MODIFY the alpha value.\n","\n","    Question (a)\n","\n","    \"\"\"\n","    ##### YOUR CODE #####\n","    pass\n","    #####################\n","\n","\n","def softmax(X):\n","    \"\"\"\n","    Implement the softmax function.\n","    The method takes the input X and returns the output of the function.\n","\n","    Question (a)\n","\n","    \"\"\"\n","    ##### YOUR CODE #####\n","    pass\n","    #####################\n","\n","def deriv_elu(x, alpha=1.0):\n","    \"\"\"\n","    Implement the derivative of elu activation function.\n","    The method takes the input z and returns the output of the function.\n","    Please DO NOT MODIFY the alpha value.\n","\n","    Question (a)\n","\n","    \"\"\"\n","    ##### YOUR CODE #####\n","    pass\n","    #####################\n","\n","def load_batch(X, Y, batch_size, shuffle=True):\n","    \"\"\"\n","    Generates batches with the remainder dropped.\n","\n","    Do NOT modify this function\n","    \"\"\"\n","    if shuffle:\n","        permutation = np.random.permutation(X.shape[0])\n","        X = X[permutation, :]\n","        Y = Y[permutation, :]\n","    num_steps = int(X.shape[0])//batch_size\n","    step = 0\n","    while step<num_steps:\n","        X_batch = X[batch_size*step:batch_size*(step+1)]\n","        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n","        step+=1\n","        yield X_batch, Y_batch"]},{"cell_type":"markdown","metadata":{"id":"tsU8v_6khR30"},"source":["# 2-Layer Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mA5udiGmhRb5"},"outputs":[],"source":["class TwoLayerNN:\n","    \"\"\" a neural network with 2 layers \"\"\"\n","\n","    def __init__(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        Do NOT modify this function.\n","        \"\"\"\n","        self.input_dim = input_dim\n","        self.num_hiddens = num_hiddens\n","        self.num_classes = num_classes\n","        self.params = self.initialize_parameters(input_dim, num_hiddens, num_classes)\n","\n","    def initialize_parameters(self, input_dim, num_hiddens, num_classes):\n","        \"\"\"\n","        initializes parameters with Xavier Initialization.\n","\n","        Question (b)\n","        - refer to https://paperswithcode.com/method/xavier-initialization for Xavier initialization\n","\n","        Inputs\n","        - input_dim\n","        - num_hiddens\n","        - num_classes\n","        Returns\n","        - params: a dictionary with the initialized parameters.\n","        \"\"\"\n","        params = {}\n","        ##### YOUR CODE #####\n","        pass\n","        #####################\n","        return params\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Define and perform the feed forward step of a two-layer neural network.\n","        Specifically, the network structue is given by\n","\n","          y = softmax(tanh(X W1 + b1) W2 + b2)\n","\n","        where X is the input matrix of shape (N, D), y is the class distribution matrix\n","        of shape (N, C), N is the number of examples (either the entire dataset or\n","        a mini-batch), D is the feature dimensionality, and C is the number of classes.\n","\n","        Question (c)\n","        - ff_dict will be used to run backpropagation in backward method.\n","\n","        Inputs\n","        - X: the input matrix of shape (N, D)\n","\n","        Returns\n","        - y: the output of the model\n","        - ff_dict: a dictionary with all the fully connected units and activations.\n","        \"\"\"\n","        ff_dict = {}\n","        ##### YOUR CODE #####\n","        pass\n","        #####################\n","        return y, ff_dict\n","\n","    def backward(self, X, Y, ff_dict):\n","        \"\"\"\n","        Performs backpropagation over the two-layer neural network, and returns\n","        a dictionary of gradients of all model parameters.\n","\n","        Question (d)\n","\n","        Inputs:\n","         - X: the input matrix of shape (B, D), where B is the number of examples\n","              in a mini-batch, D is the feature dimensionality.\n","         - Y: the matrix of one-hot encoded ground truth classes of shape (B, C),\n","              where B is the number of examples in a mini-batch, C is the number\n","              of classes.\n","         - ff_dict: the dictionary containing all the fully connected units and\n","              activations.\n","\n","        Returns:\n","         - grads: a dictionary containing the gradients of corresponding weights and biases.\n","        \"\"\"\n","        grads = {}\n","        ##### YOUR CODE #####\n","        pass\n","        #####################\n","        return grads\n","\n","\n","    def compute_loss(self, Y, Y_hat):\n","        \"\"\"\n","        Computes cross entropy loss.\n","\n","        Do NOT modify this function.\n","\n","        Inputs\n","            Y:\n","            Y_hat:\n","        Returns\n","            loss:\n","        \"\"\"\n","        loss = -(1/Y.shape[0]) * np.sum(np.multiply(Y, np.log(Y_hat)))\n","        return loss\n","\n","    def train(self, X, Y, X_val, Y_val, lr, n_epochs, batch_size, log_interval=1):\n","        \"\"\"\n","        Runs mini-batch gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X\n","        - Y\n","        - X_val\n","        - Y_Val\n","        - lr\n","        - n_epochs\n","        - batch_size\n","        - log_interval\n","        \"\"\"\n","        for epoch in range(n_epochs):\n","          for X_batch, Y_batch in load_batch(X, Y, batch_size):\n","              self.train_step(X_batch, Y_batch, batch_size, lr)\n","          if epoch % log_interval==0:\n","              Y_hat, ff_dict = self.forward(X)\n","              train_loss = self.compute_loss(Y, Y_hat)\n","              train_acc = self.evaluate(Y, Y_hat)\n","              Y_hat, ff_dict = self.forward(X_val)\n","              valid_loss = self.compute_loss(Y_val, Y_hat)\n","              valid_acc = self.evaluate(Y_val, Y_hat)\n","              print('epoch {:02} - train loss/acc: {:.3f} {:.3f}, valid loss/acc: {:.3f} {:.3f}'.\\\n","                    format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n","\n","    def train_step(self, X_batch, Y_batch, batch_size, lr):\n","        \"\"\"\n","        Updates the parameters using gradient descent.\n","\n","        Do NOT Modify this method.\n","\n","        Inputs\n","        - X_batch\n","        - Y_batch\n","        - batch_size\n","        - lr\n","        \"\"\"\n","        _, ff_dict = self.forward(X_batch)\n","        grads = self.backward(X_batch, Y_batch, ff_dict)\n","        self.params[\"W1\"] -= lr * grads[\"dW1\"]/batch_size\n","        self.params[\"b1\"] -= lr * grads[\"db1\"]/batch_size\n","        self.params[\"W2\"] -= lr * grads[\"dW2\"]/batch_size\n","        self.params[\"b2\"] -= lr * grads[\"db2\"]/batch_size\n","\n","    def evaluate(self, Y, Y_hat):\n","        \"\"\"\n","        Computes classification accuracy.\n","\n","        Do NOT modify this function\n","\n","        Inputs\n","        - Y: A numpy array of shape (N, C) containing the softmax outputs,\n","             where C is the number of classes.\n","        - Y_hat: A numpy array of shape (N, C) containing the one-hot encoded labels,\n","             where C is the number of classes.\n","\n","        Returns\n","            accuracy: the classification accuracy in float\n","        \"\"\"\n","        classes_pred = np.argmax(Y_hat, axis=1)\n","        classes_gt = np.argmax(Y, axis=1)\n","        accuracy = float(np.sum(classes_pred==classes_gt)) / Y.shape[0]\n","        return accuracy"]},{"cell_type":"markdown","metadata":{"id":"XXM2lWhtDYC6"},"source":["# Load MNIST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48ooR6YIxYhC"},"outputs":[],"source":["X_train, Y_train, X_test, Y_test = load_data()\n","\n","idxs = np.arange(len(X_train))\n","np.random.shuffle(idxs)\n","split_idx = int(np.ceil(len(idxs)*0.8))\n","X_valid, Y_valid = X_train[idxs[split_idx:]], Y_train[idxs[split_idx:]]\n","X_train, Y_train = X_train[idxs[:split_idx]], Y_train[idxs[:split_idx]]\n","print()\n","print('Set validation data aside')\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', Y_train.shape)\n","print('Validation data shape: ', X_valid.shape)\n","print('Validation labels shape: ', Y_valid.shape)"]},{"cell_type":"markdown","metadata":{"id":"tzw-D4Zr5xoi"},"source":["# Training & Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IlnC_rerHPaN"},"outputs":[],"source":["###\n","# Question (e)\n","# Tune the hyperparameters with validation data,\n","# and print the results by running the lines below.\n","###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTCqVT4S0Tm5"},"outputs":[],"source":["# model instantiation\n","model = TwoLayerNN(input_dim=784, num_hiddens=128, num_classes=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cWb6xg0NxOs"},"outputs":[],"source":["# train the model\n","lr, n_epochs, batch_size = 0.3, 50, 256\n","model.train(X_train, Y_train, X_valid, Y_valid, lr, n_epochs, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpPsAlXU0T_Z"},"outputs":[],"source":["# evalute the model on test data\n","Y_hat, _ = model.forward(X_test)\n","test_loss = model.compute_loss(Y_test, Y_hat)\n","test_acc = model.evaluate(Y_test, Y_hat)\n","print(\"Final test loss = {:.3f}, acc = {:.3f}\".format(test_loss, test_acc))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}