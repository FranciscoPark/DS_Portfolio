# Debug set to true in order to debug high-layer code.
# CFG Configuration
# yaml, json 외에도 두 가지 방식이 더 있긴 함.
# --arguments 뇌절 vs file 방식

# Make different config.yaml file for each models
CFG:
  TRAIN_PATH: "/home/ubuntu/kostat-ver2/data/1. 실습용자료.txt"
  TEST_PATH: "/home/ubuntu/kostat-ver2/data/2. 모델개발용자료.txt"

  DEVICE: 0
  DEBUG: false
  num_workers: 16
  num_instances: 100000
  batch_size: 128

  # Train configuration
  TRAIN_TOKENIZER: false
  PREPROCESS: false
  num_epochs: 6 # validation loss is increasing after 5 epochs
  max_token_length: 32
  learning_rate: 0.00008 # has to be set as float explicitly due to https://stackoverflow.com/questions/30458977/yaml-loads-5e-6-as-string-and-not-a-number
  weight_decay: 0.01 # https://paperswithcode.com/method/weight-decay
  adam_beta_1: 0.9
  adam_beta_2: 0.98
  epsilon: 0.000000001
  gamma: 0.8

  model_name: "monologg/koelectra-base-v3-discriminator"
  kobart_model_ckpt: /home/ubuntu/kostat-ver2/checkpoints/KoBART_RUN_Apr13_01.ckpt
  kobart_model_name: "gogamza/kobart-base-v2"
  koelectra_model_ckpt: "/home/ubuntu/kostat-ver2/checkpoints/KoElectra_RUN_Apr13_03.ckpt"
  koelectra_model_name: "monologg/koelectra-base-v3-discriminator"
  kobigbird_model_ckpt: "/home/ubuntu/kostat-ver2/checkpoints/KoBigBird_RUN_Apr13_03.ckpt"
  kobigbird_model_name: "monologg/kobigbird-bert-base"
  trainer: "lightning"

  # transformer settings
  loss: "crossentropy"
  activation_function: "gelu"
  dropout: 0.1

  # wandb settings
  user_name: "jangjk"
  project_name: "KoSTAT"
  entity_name: "poolc" # organization name: https://wandb.ai/poolc
  wandb_run_name: "KoBigBird_RUN_Apr13_03"
  evaluation_strategy: "steps"
  save_steps: 1000
  evaluation_steps: 1000

  # path settings
  ROOT_PATH: "."
  result_dir: "./results"
  saved_model_dir: "/home/ubuntu/kostat-ver2/lightning_save"
  final_model_ckpt: "/home/ubuntu/kostat-ver2/checkpoints/KoBigBird_RUN_Apr12_01.ckpt"
  logging_dir: "./logs"
  csv_filepath: "/home/ubuntu/kostat-ver2/csvs/ensemble_prediction.csv"
